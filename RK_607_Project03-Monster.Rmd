---
title: "RK_607_Project03-Monster"
author: "Raj Kumar"
date: "March 18th, 2018"
output:
  html_document:
    css: ./lab.css
    highlight: pygments
    theme: cerulean
  pdf_document: default
---
#### STEP BY STEP
#### 1. Build the initial search URL at monster.com to search by 
####    a. skillset
####    b. city
####    c. state
#### 2. Scan the results to get list of all the job urls from the search results
#### 3. Regex the exact Job Link and create a array of jobs
#### 4. Get/Create your data dictionary
#### 5. Open the Job Pages and search for the terms in your data dictionary
#### 6. Aggregate the terms to create a relative score of number of times these appeared
#### 7. Save the results in the database by date of the run - NOT COMPLETED
#### 8. Create a loop to run the program for multiple cities and multiple skills  - NOT COMPLETED




## STEP 1 : Load your libraries
```{r}
# Load the libraries
library(stringr)    #For string operations
library(rvest)      #For screen scrapper
library(tokenizers) #
library(tidyverse)  #For Tidyverse
library(RCurl)      #For File Operations
library(dplyr)      #For Manipulating the data frames
library(DT)         #For Data table package
library(curl)


```

## STEP 2 : Load the File
```{r}

# Good Practise: Set up the Working Directory when working with a file system
setwd("C:\\CUNY\\607Data\\Assignments")


##
# Variables to drive the program search
##
skill2search <- "Data Science"
city2search <- "New York"
state2search <- "NY"



# BUild Search Initial 
monsterUrlBuilder <- function(skillname, cityname, statecode){
    startUrl <- "https://www.monster.com/jobs/search/?q="
    skillname <- gsub(" ","-",skillname)
    middle0Url <- "&where="
    cityname <- gsub(" ","-",cityname)
    middle1Url <- "__2C-"
    middle2Url <- "&intcid=skr_navigation_nhpso_searchHeader"

    searchUrl <- paste(startUrl,skillname,middle0Url,cityname,middle1Url,statecode,middle2Url, sep="")
    searchUrl
}

searchPage <- monsterUrlBuilder(skill2search, city2search, state2search)

searchPage <- read_html(searchPage)

```

```{r}

#Get list of URLs from the Result HTML
#resulthtml <- html_nodes(searchPage, ".js_result_details-left")
#list_text <- html_text(resulthtml)
#result_text <- paste(list_text, collapse=" ")

#Built URL for all jobs on this results page
searchAllJobUrls <- unlist(str_extract_all(searchPage,'(job-openings\\.monster\\.com\\/)\\w.[^\\"]+'))
searchAllJobUrls <- paste("https://",searchAllJobUrls,sep = "")
searchAllJobUrls     #all job urls on the monster.com portal


##
# Fetch the list of all jobs with their links  
##

# Function to read the jobs posting and fetch the description section
    monsterJobUrlBuilder <- function(jobUrl){
        htmlJobPage <- read_html(curl(jobUrl, handle = curl::new_handle("useragent" = "Mozilla/5.0")))
        forecasthtml <- html_nodes(htmlJobPage, "#JobDescription")
        forecast <- html_text(forecasthtml)
        searchresult <- paste(forecast, collapse =" ")
        return(searchresult)
    }
    searchAllJobUrls[1]

```



```{r}
##
# My Data Dictionary
## 
skills <- c("data engineering", "hadoop", "python", "sql", "hive", "spark", "kafka", "database", "big data", "statistic", "model", "math", "physics", "engineering", "finance", "quantitative", "data", "matlab", " r ", "probability", "stochastic", "calculus", "design", "phd", "masters", "bachelors", "development", "scala", "oracle", "aws", "amazon", "google", "engine", "predict", "linear", "regression", "logistical", "seaborn", "ggplot", "shiny", "tensorflow", "nlp", "neuro", "language", "sas", "spss", "scipy", "numpy", "scikit", "dataset", "machine learning", "deep learning", "svm", "analytics", "clustering", "decision tree", "visualization", "math", "algorithms", "bayesian")


```

```{r}

##
# For loop to view jobs descriptions of each job and then compare against skills
## 

# This is vector with all the skills (unaggregated) found in the job descriptions
allValues <- c()

for(i in searchAllJobUrls){
    searchJobPage <- monsterJobUrlBuilder(i)
    values <- unlist(str_extract_all(tolower(searchJobPage), skills))
    allValues <- c(allValues, values) 
}

# Aggregate the skills
allValues
skillcount <- table(allValues)
skillcount <- data_frame(word = names(skillcount), count = as.numeric(skillcount))
skillcount %>% arrange(desc(count))

```



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
